{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Language Model\n",
    "\n",
    "Below is a diagram of the RNN computation that we will implement below. We're plugging characters into the RNN with a 1-hot encoding and expecting it to predict the next character. In this example the training data is the string \"hello\", so there are 4 letters in the vocabulary: [h,e,l,o]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnnlm.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 3291648 characters, 93 unique.\n"
     ]
    }
   ],
   "source": [
    "# data I/O\n",
    "data = open('warpeace.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print 'data has %d characters, %d unique.' % (data_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the twitching cheeks o\n"
     ]
    }
   ],
   "source": [
    "# lets sample a batch of data\n",
    "seq_length = 25 # number of characters in the batch\n",
    "p = 220000 # point in the book to sample from\n",
    "print data[p:p+seq_length] # print a chunk of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86, 22, 0, 22, 18, 87, 0, 22, 45, 88, 22, 40, 18, 88, 64, 41, 0, 40, 18, 87, 87, 42, 44, 0, 43]\n",
      "[22, 0, 22, 18, 87, 0, 22, 45, 88, 22, 40, 18, 88, 64, 41, 0, 40, 18, 87, 87, 42, 44, 0, 43, 62]\n"
     ]
    }
   ],
   "source": [
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print inputs\n",
    "print targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# lets plug the first character into the RNN\n",
    "ix_input = inputs[0]\n",
    "ix_target = targets[0]\n",
    "# encode the input character with a 1-hot representation\n",
    "x = np.zeros((vocab_size,1))\n",
    "x[ix_input] = 1\n",
    "print x.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create random starting parameters\n",
    "hidden_size = 10\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -2.19597294e-03  -1.11958403e-02  -7.96573514e-03  -8.29025315e-03\n",
      "  -1.69046852e-02   7.16480643e-03  -1.08628224e-05   1.38136549e-03\n",
      "   8.91231078e-03   1.22455580e-02]\n"
     ]
    }
   ],
   "source": [
    "# compute the hidden state\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev + bh))\n",
    "print h.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.86941995e-04   5.75478992e-05   5.45725372e-05  -4.12423671e-04\n",
      "   3.06201649e-04  -5.06931560e-04   3.19806236e-04  -3.51163814e-05\n",
      "  -3.38972137e-04  -2.39857442e-04  -8.01432457e-05  -1.04391992e-04\n",
      "  -3.15399258e-04   1.48454913e-05   4.55998798e-05   1.28931375e-04\n",
      "   4.04072882e-04  -5.75504844e-04   1.57533734e-04  -4.08258042e-04\n",
      "  -7.41755605e-05  -7.76690551e-05   1.87837950e-04  -2.79910204e-04\n",
      "  -7.96190232e-04  -5.79876645e-05   1.26730230e-04   3.95894081e-04\n",
      "   2.76955496e-04   2.59637379e-05  -4.61015050e-04  -5.14636280e-04\n",
      "  -2.56714611e-04   4.94377196e-04  -3.64149466e-04  -4.26364575e-04\n",
      "  -1.25515821e-04   9.29132184e-06   4.88944584e-05  -6.31681837e-04\n",
      "  -1.75121523e-05   9.51597096e-06   3.08227238e-04   1.29694758e-04\n",
      "   1.72591168e-04  -5.87989566e-05   2.74250548e-04   5.90320270e-05\n",
      "   1.64825617e-04   3.11431559e-04   3.48582321e-04  -4.19733486e-05\n",
      "  -2.77062543e-04  -1.10111861e-05  -1.33642742e-05  -5.99994639e-05\n",
      "   6.07084027e-04   3.51784289e-04  -3.89265870e-05  -1.03640935e-05\n",
      "   2.94412266e-04   7.75998698e-05  -2.85416619e-04   4.26053337e-04\n",
      "  -3.47809927e-04  -4.67567746e-04  -6.81980155e-05  -1.09923565e-06\n",
      "   6.26504669e-05  -2.76671607e-04   2.96818053e-04   1.87011604e-05\n",
      "   2.98832018e-04  -2.65531746e-04  -5.58804832e-04  -3.07801824e-04\n",
      "   3.75513314e-04   4.76734817e-04  -2.96633044e-04  -3.06249150e-04\n",
      "   1.22093579e-05  -1.43195488e-04  -1.58954120e-04  -1.08600298e-04\n",
      "   1.52122727e-04  -6.54969615e-04   1.60857638e-04  -9.11628405e-04\n",
      "   3.67740402e-05   2.16278686e-04  -1.98860111e-04   4.08539302e-05\n",
      "  -1.47124899e-04]\n"
     ]
    }
   ],
   "source": [
    "# compute the scores for next character\n",
    "y = np.dot(Why, h) + by\n",
    "print y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01075121  0.01075383  0.0107538   0.01074878  0.01075651  0.01074777\n",
      "  0.01075666  0.01075284  0.01074957  0.01075064  0.01075235  0.01075209\n",
      "  0.01074982  0.01075338  0.01075371  0.0107546   0.01075756  0.01074703\n",
      "  0.01075491  0.01074883  0.01075242  0.01075238  0.01075524  0.01075021\n",
      "  0.01074466  0.01075259  0.01075458  0.01075747  0.01075619  0.01075349\n",
      "  0.01074826  0.01074768  0.01075046  0.01075853  0.0107493   0.01074863\n",
      "  0.01075187  0.01075332  0.01075374  0.01074643  0.01075303  0.01075332\n",
      "  0.01075653  0.01075461  0.01075507  0.01075258  0.01075617  0.01075385\n",
      "  0.01075499  0.01075656  0.01075696  0.01075276  0.01075024  0.0107531\n",
      "  0.01075307  0.01075257  0.01075975  0.010757    0.0107528   0.0107531\n",
      "  0.01075638  0.01075405  0.01075015  0.0107578   0.01074948  0.01074819\n",
      "  0.01075248  0.0107532   0.01075389  0.01075024  0.01075641  0.01075342\n",
      "  0.01075643  0.01075036  0.01074721  0.01074991  0.01075725  0.01075834\n",
      "  0.01075003  0.01074992  0.01075335  0.01075168  0.01075151  0.01075205\n",
      "  0.01075485  0.01074617  0.01075495  0.01074342  0.01075361  0.01075554\n",
      "  0.01075108  0.01075365  0.01075163]\n",
      "probabilities sum to  1.0\n"
     ]
    }
   ],
   "source": [
    "# the scores are unnormalized log probabilities. compute the probabilities\n",
    "p = np.exp(y) / np.sum(np.exp(y))\n",
    "print p.ravel()\n",
    "print 'probabilities sum to ', p.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability assigned to the correct next character is right now:  0.0107552356218\n"
     ]
    }
   ],
   "source": [
    "print 'probability assigned to the correct next character is right now: ', p[ix_target,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cross-entropy (softmax) loss is  4.53236260838\n"
     ]
    }
   ],
   "source": [
    "loss = -np.log(p[ix_target,0])\n",
    "print 'the cross-entropy (softmax) loss is ', loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01075121  0.01075383  0.0107538   0.01074878  0.01075651  0.01074777\n",
      "  0.01075666  0.01075284  0.01074957  0.01075064  0.01075235  0.01075209\n",
      "  0.01074982  0.01075338  0.01075371  0.0107546   0.01075756  0.01074703\n",
      "  0.01075491  0.01074883  0.01075242  0.01075238 -0.98924476  0.01075021\n",
      "  0.01074466  0.01075259  0.01075458  0.01075747  0.01075619  0.01075349\n",
      "  0.01074826  0.01074768  0.01075046  0.01075853  0.0107493   0.01074863\n",
      "  0.01075187  0.01075332  0.01075374  0.01074643  0.01075303  0.01075332\n",
      "  0.01075653  0.01075461  0.01075507  0.01075258  0.01075617  0.01075385\n",
      "  0.01075499  0.01075656  0.01075696  0.01075276  0.01075024  0.0107531\n",
      "  0.01075307  0.01075257  0.01075975  0.010757    0.0107528   0.0107531\n",
      "  0.01075638  0.01075405  0.01075015  0.0107578   0.01074948  0.01074819\n",
      "  0.01075248  0.0107532   0.01075389  0.01075024  0.01075641  0.01075342\n",
      "  0.01075643  0.01075036  0.01074721  0.01074991  0.01075725  0.01075834\n",
      "  0.01075003  0.01074992  0.01075335  0.01075168  0.01075151  0.01075205\n",
      "  0.01075485  0.01074617  0.01075495  0.01074342  0.01075361  0.01075554\n",
      "  0.01075108  0.01075365  0.01075163]\n",
      "sum of dy is  -4.23272528138e-16\n",
      "the gradient for the correct character (t) is: -0.989244764378\n",
      "the gradient for the character (a) is:  0.0107549454461\n"
     ]
    }
   ],
   "source": [
    "# compute the gradient on y\n",
    "dy = np.copy(p)\n",
    "dy[ix_target] -= 1\n",
    "print dy.ravel()\n",
    "print 'sum of dy is ', dy.sum()\n",
    "print 'the gradient for the correct character (%s) is: %s' % (ix_to_char[ix_target], dy[ix_target,0])\n",
    "print 'the gradient for the character (a) is: ', dy[char_to_ix['a'],0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the hidden vector activations were:\n",
      "[ -2.19597294e-03  -1.11958403e-02  -7.96573514e-03  -8.29025315e-03\n",
      "  -1.69046852e-02   7.16480643e-03  -1.08628224e-05   1.38136549e-03\n",
      "   8.91231078e-03   1.22455580e-02]\n",
      "the gradients are:\n",
      "[ 0.010636   -0.00556332  0.01572234 -0.01013355  0.01024101  0.00893933\n",
      "  0.01156068 -0.0035879  -0.00304811 -0.00761244]\n",
      "the gradients dWhy have size:  (93, 10)\n",
      "a small sample is:\n",
      "[[ -2.36093564e-05  -1.20368781e-04  -8.56412557e-05  -8.91302155e-05]\n",
      " [ -2.36151293e-05  -1.20398213e-04  -8.56621967e-05  -8.91520096e-05]\n",
      " [ -2.36150591e-05  -1.20397855e-04  -8.56619418e-05  -8.91517444e-05]\n",
      " [ -2.36040335e-05  -1.20341643e-04  -8.56219473e-05  -8.91101206e-05]]\n"
     ]
    }
   ],
   "source": [
    "# we computed [y = np.dot(Why, h) + by]; Backpropagate to Why, h, and by\n",
    "dWhy = np.dot(dy, h.T)\n",
    "dh = np.dot(Why.T, dy)\n",
    "dby = np.copy(dy)\n",
    "print 'the hidden vector activations were:'\n",
    "print h.ravel()\n",
    "print 'the gradients are:'\n",
    "print dh.ravel()\n",
    "print 'the gradients dWhy have size: ', dWhy.shape\n",
    "print 'a small sample is:'\n",
    "print dWhy[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small sample of Whh:\n",
      "[[-0.0181644  -0.00730641  0.00068086  0.00043998]\n",
      " [-0.00706002 -0.00657655  0.0100175   0.00198405]\n",
      " [ 0.0175388   0.01643369 -0.00161146 -0.01413518]\n",
      " [ 0.00812143 -0.00105367  0.00944295  0.01148334]]\n"
     ]
    }
   ],
   "source": [
    "# we computed [h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h_prev + bh))]; \n",
    "# Backprop into Wxh, x, Whh, h_prev, bh:\n",
    "dh_before_tanh = (1-h**2)*dh\n",
    "dbh = np.copy(dh_before_tanh)\n",
    "dWxh = np.dot(dh_before_tanh, x.T)\n",
    "dWhh = np.dot(dh_before_tanh, h.T)\n",
    "dh_prev = np.dot(Whh.T, dh_before_tanh)\n",
    "print 'small sample of Whh:'\n",
    "print Whh[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we now have the gradients for all parameters! (Wxh, Whh, Why, bh, by)\n",
    "# lets do a parameter update\n",
    "learning_rate = 0.1\n",
    "Wxh2 = Wxh - learning_rate * dWxh\n",
    "Whh2 = Whh - learning_rate * dWhh\n",
    "Why2 = Why - learning_rate * dWhy\n",
    "bh2 = bh - learning_rate * dbh\n",
    "by2 = by - learning_rate * dby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability assigned to the correct next character was:  0.0107552356218\n",
      "probability assigned to the correct next character is now:  0.0118750046895\n",
      "the cross-entropy (softmax) loss was  4.53236260838\n",
      "the loss is now  4.43331953415\n"
     ]
    }
   ],
   "source": [
    "# these parameters should be much better! lets try it out:\n",
    "h2 = np.tanh(np.dot(Wxh2, x) + np.dot(Whh2, h_prev + bh2))\n",
    "y2 = np.dot(Why2, h2) + by2\n",
    "p2 = np.exp(y2) / np.sum(np.exp(y2))\n",
    "print 'probability assigned to the correct next character was: ', p[ix_target,0]\n",
    "print 'probability assigned to the correct next character is now: ', p2[ix_target,0]\n",
    "loss2 = -np.log(p2[ix_target,0])\n",
    "print 'the cross-entropy (softmax) loss was ', loss\n",
    "print 'the loss is now ', loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note: the probability for the correct character went up! (and the loss went down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# putting it together with loops\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass\n",
    "    for t in xrange(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "        ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        \n",
    "    # clip to mitigate exploding gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113.314839864\n"
     ]
    }
   ],
   "source": [
    "loss, dWxh, dWhh, dWhy, dbh, dby, hnew = lossFun(inputs, targets, h_prev)\n",
    "print loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: write the sampling code\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\" \n",
    "    sample a sequence of integers from the model \n",
    "    h is initial memory state, seed_ix is seed letter for first time step\n",
    "    n is the number of time steps to sample for\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = [] # sampled indices\n",
    "    for t in xrange(n):\n",
    "        pass # TODO: run the RNN for one time step, sample from distribution\n",
    "    return ixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: write the optimization loop\n",
    "# Loop over the dataset from beginning to end, sampling batches of characters seq_length long\n",
    "# Call the loss function and get the gradients\n",
    "# Perform a parameter update\n",
    "# Sample some examples from the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
